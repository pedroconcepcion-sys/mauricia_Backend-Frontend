import os
import sys
from dotenv import load_dotenv

# --- LIBRER√çAS DE LANGCHAIN ---
from langchain_ollama import ChatOllama  # Para el cerebro local
from langchain_openai import OpenAIEmbeddings # Para leer tu DB actual
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# 1. Cargar variables de entorno (.env)
load_dotenv()

# --- CONFIGURACI√ìN ---
CARPETA_DB = "chroma_db_prod" # Usamos la carpeta que ya creaste
MODELO_OLLAMA = "llama3.1"    # Aseg√∫rate de tenerlo instalado: ollama pull llama3.1
MODELO_EMBEDDINGS = "text-embedding-3-small"

def iniciar_mauricia_local():
    print("üè† Iniciando MauricIA en MODO LOCAL...")

    # --- A) CARGAR MODELO DE EMBEDDINGS ---
    # Necesitamos esto porque tu base de datos 'chroma_db_prod' fue creada con OpenAI.
    # Si us√°ramos HuggingFace aqu√≠, dar√≠a error de dimensiones.
    try:
        embedding_function = OpenAIEmbeddings(
            model=MODELO_EMBEDDINGS,
            api_key=os.getenv("GITHUB_TOKEN"),
            base_url="https://models.inference.ai.azure.com"
        )
        print("‚úÖ Embeddings configurados (Azure/OpenAI).")
    except Exception as e:
        print(f"‚ùå Error configurando Embeddings: {e}")
        sys.exit(1)

    # --- B) CONECTAR A LA BASE DE DATOS VECTORIAL ---
    if not os.path.exists(CARPETA_DB):
        print(f"‚ùå ERROR: No encuentro la carpeta '{CARPETA_DB}'.")
        print("   Ejecuta primero el script 'crear_cerebro_cloud.py' para generarla.")
        sys.exit(1)

    vector_db = Chroma(
        persist_directory=CARPETA_DB,
        embedding_function=embedding_function
    )
    print(f"‚úÖ Base de datos '{CARPETA_DB}' cargada correctamente.")

    # --- C) CONFIGURAR EL LLM LOCAL (OLLAMA) ---
    print(f"ü¶ô Conectando con Ollama ({MODELO_OLLAMA})...")
    try:
        llm = ChatOllama(
            model=MODELO_OLLAMA,
            temperature=0.0, # Creatividad baja para ser precisos
            base_url="http://localhost:11434"
        )
    except Exception as e:
        print(f"‚ùå Error conectando con Ollama. ¬øEst√° corriendo la app?: {e}")
        sys.exit(1)

    # --- D) CREAR EL PROMPT (PERSONALIDAD) ---
    system_prompt = (
        "Eres MauricIA, una asistente experta en los programas de postgrado del Departamento de Ingenier√≠a Inform√°tica de la USACH. "
        "Usa los siguientes fragmentos de contexto recuperado para responder la pregunta del usuario. "
        "Si no sabes la respuesta, di que no tienes esa informaci√≥n. No inventes datos. "
        "Responde de manera amable, formal y concisa.\n\n"
        "{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    # --- E) ARMAR LA CADENA RAG (CEREBRO + MEMORIA) ---
    try:
        # 1. Cadena que procesa los documentos
        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        
        # 2. Cadena que busca y luego responde
        rag_chain = create_retrieval_chain(vector_db.as_retriever(), question_answer_chain)
        print("‚úÖ ¬°Sistema MauricIA Local LISTO! üöÄ\n")
        return rag_chain
    except Exception as e:
        print(f"‚ùå Error armando la cadena RAG: {e}")
        sys.exit(1)

# --- BLOQUE DE EJECUCI√ìN ---
if __name__ == "__main__":
    app_rag = iniciar_mauricia_local()
    
    print("üí¨ Escribe 'salir' para terminar.")
    print("--------------------------------------------------")

    while True:
        pregunta = input("\nüë§ T√∫: ")
        if pregunta.lower() in ["salir", "exit", "chau"]:
            print("üëã ¬°Hasta luego!")
            break
        
        # Invocar al agente
        print("ü§ñ Pensando...", end="\r")
        try:
            respuesta = app_rag.invoke({"input": pregunta})
            print(f"ü§ñ MauricIA: {respuesta['answer']}")
            
            # (Opcional) Ver qu√© documentos ley√≥:
            # for i, doc in enumerate(respuesta["context"]):
            #     print(f"   [Fuente {i+1}]: {doc.metadata.get('source', 'Desconocido')}")

        except Exception as e:
            print(f"‚ùå Error generando respuesta: {e}")