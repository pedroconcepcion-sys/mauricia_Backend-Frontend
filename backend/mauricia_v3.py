import os
import time
import re
import sys
from dotenv import load_dotenv

# --- IMPORTS LIGEROS PARA PRODUCCIÃ“N ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Cambio clave aquÃ­
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory

# =============================================================================
# 0. CONFIGURACIÃ“N INICIAL
# =============================================================================
load_dotenv()

# Usamos la carpeta que generaste con OpenAI
CARPETA_DB = "chroma_db_prod" 
# Modelo de OpenAI: rÃ¡pido, barato y no consume RAM en el servidor
MODELO_EMBEDDINGS = "text-embedding-3-small"
SESSION_ID = "sesion_usuario_local"  

MAX_CONTEXT_CHARS = 12000  
K_NORMAL = 4              
K_DINERO = 10 

# =============================================================================
# 1. LAZY LOADING: VARIABLES GLOBALES
# =============================================================================
sistema_cargado = False
vector_db = None
conversational_rag_chain = None
store = {} 

# =============================================================================
# 2. PROMPT DEL SISTEMA (Tu lÃ³gica original intacta)
# =============================================================================
SYSTEM_PROMPT_V3 = (
    "Eres MauricIA, la asistente oficial de Postgrados USACH.\n"
    "Tus instrucciones son INVIOLABLES. Responde basÃ¡ndote en el CONTEXTO y el HISTORIAL.\n"
    "\n"
    "ðŸ§  PROTOCOLO DE RAZONAMIENTO (NO IMPRIMIR):\n"
    "1. ANALIZA EL HISTORIAL MENTALMENTE: Revisa si el usuario ya mencionÃ³ un programa.\n"
    "2. DETECCIÃ“N DE AMBIGÃœEDAD: Si no sabes el programa, pregunta.\n"
    "â›” PROHIBICIONES DE FORMATO: NO uses etiquetas como 'Respuesta:', 'Paso 1:'.\n"
    "\n"
    "ðŸš¨ REGLAS DE SEGURIDAD:\n"
    "- â›” NO ACADÃ‰MICO: Recetas, gym, clima -> 'No tengo informaciÃ³n sobre eso'.\n"
    "- âœ… INFORMACIÃ“N VÃLIDA: Costos, Mallas, Becas, Requisitos y CONTACTO.\n"
    "- ðŸ“ Si preguntan por Profesores o LÃ­neas de investigaciÃ³n: responde que estarÃ¡ pronto en el contexto.\n"
    "- ðŸ“ Nota mÃ­nima pregrado: responde que no influye.\n"
    "- ðŸ“ Co-tutela o carrera distinta: responde que SÃ es posible.\n"
    "ðŸ’° REGLAS FINANCIERAS:\n"
    "- MATRÃCULA (~$167.000) != ARANCEL (Millones).\n"
    "- PROHIBIDO MULTIPLICAR o sumar valores.\n"
    "ðŸ“ FORMATO: Respuesta directa, cÃ¡lida, usa viÃ±etas y entrega LINKS si hay."
)

RESP_NO_ACADEMICO = "No tengo informaciÃ³n sobre servicios no acadÃ©micos, solo sobre postgrados."
RESP_BLOQUEO = "Lo siento, solo puedo responder consultas sobre Postgrados USACH."

INYECCION_PROHIBIDA = ["ignora", "ignore", "olvida", "jailbreak", "modo desarrollador"]
NO_ACADEMICO_KW = ["receta", "cocina", "pizza", "sushi", "chiste", "clima", "piscina", "gym", "casino"]
SALUDOS_KW = {"hola", "holi", "buenas", "buenos", "dias", "saludos", "hey", "que", "tal", "mauricia"}
KW_DINERO = ("cuanto", "precio", "valor", "costo", "sale", "arancel", "matricula")

_re_inyeccion = re.compile("|".join(re.escape(x) for x in INYECCION_PROHIBIDA), re.IGNORECASE)
_re_noacad = re.compile("|".join(re.escape(x) for x in NO_ACADEMICO_KW), re.IGNORECASE)

# =============================================================================
# 3. FUNCIONES AUXILIARES
# =============================================================================
def es_saludo_puro(user_input: str) -> bool:
    t = re.sub(r'[^\w\s]', '', (user_input or "").lower().strip())
    words = t.split()
    return len(words) < 6 and any(w in SALUDOS_KW for w in words)

def es_consulta_dinero(user_input: str) -> bool:
    return any(k in (user_input or "").lower() for k in KW_DINERO)

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# =============================================================================
# 4. INICIALIZACIÃ“N LIGERA (OPENAI CLOUD)
# =============================================================================
def inicializar_sistema():
    global vector_db, conversational_rag_chain, sistema_cargado
    
    print("â˜ï¸ Conectando con el cerebro en la nube (OpenAI Mode)...")
    
    api_key = os.getenv("GITHUB_TOKEN")
    if not api_key:
        print("âŒ Error: GITHUB_TOKEN no configurado.")
        return False
    
    try:
        # 1. Cargar LLM (GPT-4o mini)
        llm = ChatOpenAI(
            base_url=os.getenv("OPENAI_BASE_URL"),
            model=os.getenv("MODEL_NAME"),
            api_key=api_key,
            temperature=0.0,
            max_tokens=300
        )

        # 2. Embeddings de OpenAI (No consumen RAM local)
        embedding_function = OpenAIEmbeddings(
            model=MODELO_EMBEDDINGS,
            api_key=os.getenv("GITHUB_TOKEN"),
            base_url="https://models.inference.ai.azure.com"
        )
        
        # 3. Conectar ChromaDB
        if os.path.exists(CARPETA_DB):
            vector_db = Chroma(
                persist_directory=CARPETA_DB,
                embedding_function=embedding_function
            )
            print("âœ… ChromaDB (OpenAI) conectado.")
        else:
            print(f"âŒ Error: No existe la carpeta {CARPETA_DB}")
            return False
            
        # 4. Construir Cadena RAG
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT_V3),
            MessagesPlaceholder(variable_name="chat_history"), 
            ("human", "CONTEXTO RECUPERADO:\n{context}\n\nPREGUNTA DEL USUARIO:\n{input}")
        ])
        
        chain = qa_prompt | llm | StrOutputParser()
        
        conversational_rag_chain = RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
        )
        
        sistema_cargado = True
        return True

    except Exception as e:
        print(f"âŒ Error en inicializaciÃ³n: {e}")
        return False

# =============================================================================
# 5. OBTENER RESPUESTA (Con Lazy Loading)
# =============================================================================
def obtener_respuesta_agente(user_input: str, session_id: str = SESSION_ID) -> str:
    global sistema_cargado
    
    user_input = (user_input or "").strip()
    if not user_input: return "..."

    if _re_inyeccion.search(user_input): return RESP_BLOQUEO
    if _re_noacad.search(user_input): return RESP_NO_ACADEMICO
    
    if es_saludo_puro(user_input):
        return "Â¡Hola! Soy MauricIA, tu asistente de Postgrados USACH. Â¿Sobre quÃ© programa te gustarÃ­a informarte hoy?"

    if not sistema_cargado:
        if not inicializar_sistema():
            return "âš ï¸ El cerebro estÃ¡ teniendo problemas para iniciar. Revisa los logs."

    try:
        k_val = K_DINERO if es_consulta_dinero(user_input) else K_NORMAL
        query_search = user_input
        if es_consulta_dinero(user_input):
            query_search += " arancel matrÃ­cula costo valor"

        # BÃºsqueda
        docs = vector_db.similarity_search(query_search, k=k_val)
        
        contexto_str = "\n\n".join([d.page_content for d in docs])
        if len(contexto_str) > MAX_CONTEXT_CHARS:
            contexto_str = contexto_str[:MAX_CONTEXT_CHARS]

        # InvocaciÃ³n
        respuesta = conversational_rag_chain.invoke(
            {"input": user_input, "context": contexto_str},
            config={"configurable": {"session_id": session_id}}
        )
        return respuesta

    except Exception as e:
        print(f"Error: {e}")
        return "Lo siento, tuve un problema procesando tu solicitud. Â¿PodrÃ­as intentar de nuevo?"

if __name__ == "__main__":
    print("\nðŸŽ“ MAURICIA CLOUD READY")
    while True:
        txt = input("\nðŸ§‘ TÃº: ")
        if txt.lower() == "salir": break
        print("ðŸ¤– MauricIA:", obtener_respuesta_agente(txt))